import pickle as pickle
import os
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer
import numpy as np
import random
from data.dataset import RE_Dataset
from utils.metrics import compute_metrics
from model.model import load_model
from preprocessing.tokenizer import TypedEntityMarkerPuncTokenizer

import wandb

# ì‹œë“œ ì„¤ì •
def set_seed(seed:int = 42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)

def train():
    """
    KLUE ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ê³„ ì¶”ì¶œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì„¤ì •í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ í›ˆë ¨ ì¸ìë¥¼ ì§€ì •í•œ ë‹¤ìŒ, íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Trainerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

    í›ˆë ¨ëœ ëª¨ë¸ì€ './best_model' ë””ë ‰í„°ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.
    
    ì°¸ê³  ìë£Œ:
        - í—ˆê¹… í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: https://huggingface.co/transformers/
        - KLUE ê´€ê³„ ì¶”ì¶œ ë°ì´í„° ì„¸íŠ¸: https://klue-benchmark.com/tasks/67/overview
        - í—ˆê¹… í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ training options: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
        
    Raises:
        ImportError: transformers, torch, numpy, ë˜ëŠ” sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤.
    """
    # wandb init
    wandb.init()

    # ì‹œë“œ ì„¤ì •
    set_seed(42)
    # ëª¨ë¸, í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
    MODEL_NAME = "klue/bert-base"
    tokenizer = TypedEntityMarkerPuncTokenizer(MODEL_NAME)

    # load dataset
    RE_train_dataset = RE_Dataset(data_path="./dataset/train/train_split_v1.csv", 
                                  tokenizer=tokenizer)
    RE_valid_dataset = RE_Dataset(data_path="./dataset/valid/valid_split_v1.csv", 
                                  tokenizer=tokenizer)

    # ë””ë°”ì´ìŠ¤ì— ì˜¬ë¦¬ê¸°
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    # print(device)
    
    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = load_model(MODEL_NAME, num_labels=30)
    model.to(device)
    model.resize_token_embeddings(len(tokenizer.tokenizer))
    # print(model.config)
    
    # í›ˆë ¨ ì¸ì ì§€ì •
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        save_total_limit=5,              # number of total save model.
        save_steps=40,                 # model saving step.
        num_train_epochs=3,              # total number of training epochs
        learning_rate=wandb.config.lr,               # learning_rate
        per_device_train_batch_size=wandb.config.batch_size,  # batch size per device during training
        per_device_eval_batch_size=16,   # batch size for evaluation
        warmup_steps=5,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=20,              # log saving step.
        evaluation_strategy='steps', # evaluation strategy to adopt during training
                                    # `no`: No evaluation during training.
                                    # `steps`: Evaluate every `eval_steps`.
                                    # `epoch`: Evaluate every end of epoch.
        eval_steps = 20,            # evaluation step.
        load_best_model_at_end = True,
        report_to='wandb',
    )
    # Trainer ì„ ì–¸
    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=RE_train_dataset,         # training dataset
        eval_dataset=RE_valid_dataset,             # evaluation dataset
        compute_metrics=compute_metrics         # define metrics function
    )

    # train í•¨ìˆ˜ ì‹¤í–‰
    trainer.train()
    model.save_pretrained('./best_model')
    
def main():
    # wandb sweepí•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    sweep_config = {    # yaml íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
        'method': 'random',
        'metric': {
            'name':'eval/micro f1 score', 
            'goal':'maximize'
        },
        'parameters': {
            'lr':{
                'distribution': 'uniform',  
                'min':5e-5,                 
                'max':5e-2                 
            },
            'batch_size': {
                'values': [16, 32, 64]
            },
            # 'warmup_steps': {
            #     'values': [0, 100, 500]
            # }, 
            # 'epochs': {
            #     'values': [5, 10, 20]
            # }
        }        
    }
    # sweep_id ìƒì„±
    sweep_id = wandb.sweep(
        project='KLUE-RE',
        sweep=sweep_config,  
    )

    # sweep agent ìƒì„±
    wandb.agent(
        sweep_id=sweep_id,      
        function=train,   
        count=3 # yaml íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°          
    )

if __name__ == '__main__':
    main()
