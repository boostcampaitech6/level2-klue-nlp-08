import pickle as pickle
import os
import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, Trainer, TrainingArguments, RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer
import numpy as np
import random
from load_data import *
from label_utils import *
from metrics import compute_metrics
from model import load_model

from tokenizing import *
from preprocessing import *

# ì‹œë“œ ì„¤ì •
def set_seed(seed:int = 42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)

def train():
    """
    KLUE ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ê³„ ì¶”ì¶œ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì„¤ì •í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ í›ˆë ¨ ì¸ìë¥¼ ì§€ì •í•œ ë‹¤ìŒ, íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Trainerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

    í›ˆë ¨ëœ ëª¨ë¸ì€ './best_model' ë””ë ‰í„°ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.
    
    ì°¸ê³  ìë£Œ:
        - í—ˆê¹… í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: https://huggingface.co/transformers/
        - KLUE ê´€ê³„ ì¶”ì¶œ ë°ì´í„° ì„¸íŠ¸: https://klue-benchmark.com/tasks/67/overview
        - í—ˆê¹… í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ training options: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
        
    Raises:
        ImportError: transformers, torch, numpy, ë˜ëŠ” sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤.
    """
    # ì‹œë“œ ì„¤ì •
    set_seed(42)
    # ëª¨ë¸, í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
    MODEL_NAME = "klue/bert-base"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # load dataset
    train_dataset = load_data("../dataset/train/train.csv")
    # dev_dataset = load_data("../dataset/train/dev.csv") # validationìš© ë°ì´í„°ëŠ” ë”°ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤.

    train_label = label_to_num(train_dataset['label'].values)
    # dev_label = label_to_num(dev_dataset['label'].values)

    # tokenizing dataset
    tokenized_train = tokenized_dataset(train_dataset, tokenizer)
    # tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)

    # make dataset for pytorch.
    RE_train_dataset = RE_Dataset(tokenized_train, train_label)
    # RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)

    # ë””ë°”ì´ìŠ¤ì— ì˜¬ë¦¬ê¸°
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)
    
    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = load_model(MODEL_NAME, num_labels=30)
    model.to(device)
    print(model.config)
    
    # í›ˆë ¨ ì¸ì ì§€ì •
    training_args = TrainingArguments(
        output_dir='./results',          # output directory
        save_total_limit=5,              # number of total save model.
        save_steps=500,                 # model saving step.
        num_train_epochs=20,              # total number of training epochs
        learning_rate=5e-5,               # learning_rate
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=16,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir='./logs',            # directory for storing logs
        logging_steps=100,              # log saving step.
        evaluation_strategy='steps', # evaluation strategy to adopt during training
                                    # `no`: No evaluation during training.
                                    # `steps`: Evaluate every `eval_steps`.
                                    # `epoch`: Evaluate every end of epoch.
        eval_steps = 500,            # evaluation step.
        load_best_model_at_end = True 
    )
    # Trainer ì„ ì–¸
    trainer = Trainer(
        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=RE_train_dataset,         # training dataset
        eval_dataset=RE_train_dataset,             # evaluation dataset
        compute_metrics=compute_metrics         # define metrics function
    )

    # train í•¨ìˆ˜ ì‹¤í–‰
    trainer.train()
    model.save_pretrained('./best_model')
    
def main():
    train()

if __name__ == '__main__':
    main()
